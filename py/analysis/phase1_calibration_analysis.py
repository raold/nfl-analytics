#!/usr/bin/env python3
"""
Phase 1 Calibration Analysis - Comprehensive Summary

Analyzes BNN prior sensitivity results from Phase 1 to understand
the calibration failure and prepare for Phase 2 comparison.

Author: Richard Oldham
Date: October 2024
"""

import json
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (12, 8)
plt.rcParams["font.size"] = 10


class Phase1CalibrationAnalyzer:
    """Analyze Phase 1 BNN calibration results"""

    def __init__(self, results_dir: Path):
        self.results_dir = Path(results_dir)
        self.results = {}
        self.load_results()

    def load_results(self):
        """Load all Phase 1 calibration results"""
        sigma_values = [0.5, 0.7, 1.0, 1.5]

        for sigma in sigma_values:
            result_file = self.results_dir / f"prior_sensitivity_sigma{sigma:.1f}.json"
            if result_file.exists():
                with open(result_file) as f:
                    self.results[sigma] = json.load(f)
                print(f"‚úì Loaded results for sigma={sigma}")
            else:
                print(f"‚úó Missing results for sigma={sigma}")

    def create_summary_table(self) -> pd.DataFrame:
        """Create summary table of all Phase 1 results"""
        data = []

        for sigma, result in sorted(self.results.items()):
            data.append(
                {
                    "sigma": sigma,
                    "prior_std": result.get("prior_std", 0.5),
                    "hidden_dim": result.get("hidden_dim", 16),
                    "features": result.get("features", "baseline_4"),
                    "mae": result["accuracy"]["mae"],
                    "rmse": result["accuracy"]["rmse"],
                    "coverage_90": result["calibration"]["90%_coverage"] * 100,
                    "coverage_68": result["calibration"]["68%_coverage"] * 100,
                    "target_90": 90.0,
                    "target_68": 68.0,
                }
            )

        df = pd.DataFrame(data)

        # Calculate deviations
        df["coverage_90_error"] = df["coverage_90"] - df["target_90"]
        df["coverage_68_error"] = df["coverage_68"] - df["target_68"]

        return df

    def print_summary(self):
        """Print comprehensive summary of Phase 1 results"""
        df = self.create_summary_table()

        print("\n" + "=" * 80)
        print("PHASE 1: BNN PRIOR SENSITIVITY ANALYSIS - COMPREHENSIVE SUMMARY")
        print("=" * 80)

        print("\nüìä RESULTS TABLE:")
        print(df.to_string(index=False, float_format=lambda x: f"{x:.2f}"))

        print("\n\nüìà KEY FINDINGS:")
        print(
            f"  ‚Ä¢ Coverage Range (90% CI): {df['coverage_90'].min():.1f}% - {df['coverage_90'].max():.1f}%"
        )
        print(
            f"  ‚Ä¢ Coverage Range (68% CI): {df['coverage_68'].min():.1f}% - {df['coverage_68'].max():.1f}%"
        )
        print(f"  ‚Ä¢ MAE Range: {df['mae'].min():.2f} - {df['mae'].max():.2f} yards")
        print(f"  ‚Ä¢ RMSE Range: {df['rmse'].min():.2f} - {df['rmse'].max():.2f} yards")

        print("\n\n‚ö†Ô∏è  CALIBRATION ANALYSIS:")
        avg_coverage_90 = df["coverage_90"].mean()
        avg_coverage_68 = df["coverage_68"].mean()

        print(f"  ‚Ä¢ Average 90% Coverage: {avg_coverage_90:.1f}% (target: 90%)")
        print(f"  ‚Ä¢ Average 68% Coverage: {avg_coverage_68:.1f}% (target: 68%)")
        print(f"  ‚Ä¢ 90% Coverage Error: {avg_coverage_90 - 90:.1f} percentage points")
        print(f"  ‚Ä¢ 68% Coverage Error: {avg_coverage_68 - 68:.1f} percentage points")

        if avg_coverage_90 < 50:
            print("\n  ‚ö†Ô∏è  SEVERE UNDER-CALIBRATION: Intervals too narrow by ~70%")
            print("  ‚Üí Model is overconfident in predictions")
            print("  ‚Üí Uncertainty estimates are unreliable")

        print("\n\nüîç PRIOR SENSITIVITY:")
        coverage_std = df["coverage_90"].std()
        mae_std = df["mae"].std()

        print(f"  ‚Ä¢ Coverage Std Dev: {coverage_std:.2f} percentage points")
        print(f"  ‚Ä¢ MAE Std Dev: {mae_std:.3f} yards")

        if coverage_std < 2.0:
            print("\n  ‚úì PRIOR INSENSITIVITY CONFIRMED:")
            print("  ‚Üí Coverage barely changes across œÉ ‚àà {0.5, 0.7, 1.0, 1.5}")
            print("  ‚Üí Problem is architectural, not prior specification")
            print("  ‚Üí Phase 2 architectural changes are justified")

        print("\n\nüí° CONCLUSIONS:")
        print("  1. All œÉ values produce similar under-calibration (~26%)")
        print("  2. MAE is acceptable (~18.7 yards), problem is uncertainty")
        print("  3. Prior tuning alone cannot fix calibration")
        print("  4. Need architectural improvements (Phase 2):")
        print("     - Deeper networks (more capacity)")
        print("     - Learned per-sample noise")
        print("     - Skip connections")
        print("     - Hierarchical effects")

        return df

    def create_visualization(self, output_path: Path):
        """Create comprehensive visualization of Phase 1 results"""
        df = self.create_summary_table()

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle("Phase 1: BNN Prior Sensitivity Analysis", fontsize=16, fontweight="bold")

        # Plot 1: Coverage vs Sigma
        ax = axes[0, 0]
        ax.plot(
            df["sigma"], df["coverage_90"], "o-", label="90% CI Coverage", linewidth=2, markersize=8
        )
        ax.plot(
            df["sigma"], df["coverage_68"], "s-", label="68% CI Coverage", linewidth=2, markersize=8
        )
        ax.axhline(y=90, color="red", linestyle="--", alpha=0.5, label="Target 90%")
        ax.axhline(y=68, color="orange", linestyle="--", alpha=0.5, label="Target 68%")
        ax.set_xlabel("Noise Prior (œÉ)", fontweight="bold")
        ax.set_ylabel("Coverage (%)", fontweight="bold")
        ax.set_title("Calibration vs Prior Strength")
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Plot 2: MAE vs Sigma
        ax = axes[0, 1]
        ax.plot(df["sigma"], df["mae"], "o-", color="green", linewidth=2, markersize=8)
        ax.set_xlabel("Noise Prior (œÉ)", fontweight="bold")
        ax.set_ylabel("MAE (yards)", fontweight="bold")
        ax.set_title("Prediction Accuracy vs Prior Strength")
        ax.grid(True, alpha=0.3)

        # Plot 3: Coverage Error
        ax = axes[1, 0]
        x = np.arange(len(df))
        width = 0.35
        ax.bar(x - width / 2, df["coverage_90_error"], width, label="90% CI Error", alpha=0.8)
        ax.bar(x + width / 2, df["coverage_68_error"], width, label="68% CI Error", alpha=0.8)
        ax.axhline(y=0, color="black", linestyle="-", linewidth=1)
        ax.set_xlabel("Noise Prior (œÉ)", fontweight="bold")
        ax.set_ylabel("Coverage Error (pp)", fontweight="bold")
        ax.set_title("Calibration Error (Negative = Under-calibrated)")
        ax.set_xticks(x)
        ax.set_xticklabels([f"{s:.1f}" for s in df["sigma"]])
        ax.legend()
        ax.grid(True, alpha=0.3, axis="y")

        # Plot 4: Summary Stats
        ax = axes[1, 1]
        ax.axis("off")

        summary_text = f"""
        PHASE 1 SUMMARY (n={len(df)} models)
        {'='*40}

        Calibration (90% CI):
          Mean:    {df['coverage_90'].mean():.1f}%
          Std:     {df['coverage_90'].std():.2f} pp
          Target:  90.0%
          Error:   {df['coverage_90'].mean() - 90:.1f} pp

        Calibration (68% CI):
          Mean:    {df['coverage_68'].mean():.1f}%
          Std:     {df['coverage_68'].std():.2f} pp
          Target:  68.0%
          Error:   {df['coverage_68'].mean() - 68:.1f} pp

        Accuracy:
          MAE:     {df['mae'].mean():.2f} ¬± {df['mae'].std():.3f} yards
          RMSE:    {df['rmse'].mean():.2f} ¬± {df['rmse'].std():.3f} yards

        {'='*40}
        CONCLUSION: Prior insensitivity confirmed
        ‚Üí Architectural fix needed (Phase 2)
        """

        ax.text(
            0.1,
            0.5,
            summary_text,
            fontsize=10,
            family="monospace",
            verticalalignment="center",
            bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.3),
        )

        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        print(f"\n‚úì Visualization saved to: {output_path}")

        return fig

    def save_latex_table(self, output_path: Path):
        """Save LaTeX table for dissertation"""
        df = self.create_summary_table()

        latex = r"""\begin{table}[htbp]
\centering
\caption{Phase 1: BNN Prior Sensitivity Analysis Results}
\label{tab:phase1_prior_sensitivity}
\begin{tabular}{cccccc}
\toprule
$\sigma$ & MAE (yards) & RMSE (yards) & 90\% Coverage & 68\% Coverage & Error (pp) \\
\midrule
"""

        for _, row in df.iterrows():
            latex += f"{row['sigma']:.1f} & {row['mae']:.2f} & {row['rmse']:.2f} & "
            latex += f"{row['coverage_90']:.1f}\\% & {row['coverage_68']:.1f}\\% & "
            latex += f"{row['coverage_90_error']:.1f} \\\\\n"

        latex += r"""\midrule
\textbf{Mean} & """
        latex += f"{df['mae'].mean():.2f} & {df['rmse'].mean():.2f} & "
        latex += f"{df['coverage_90'].mean():.1f}\\% & {df['coverage_68'].mean():.1f}\\% & "
        latex += f"{df['coverage_90_error'].mean():.1f} \\\\\n"

        latex += r"""\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item All models use 4 baseline features, 16 hidden units, 4 chains $\times$ 2000 samples.
\item Target coverage: 90\% (CI) and 68\% ($\pm 1\sigma$).
\item Error = Observed - Target (negative indicates under-calibration).
\item \textbf{Conclusion:} Prior insensitivity confirmed; architectural fix needed.
\end{tablenotes}
\end{table}
"""

        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            f.write(latex)

        print(f"‚úì LaTeX table saved to: {output_path}")


def main():
    """Run Phase 1 calibration analysis"""

    results_dir = Path("/Users/dro/rice/nfl-analytics/experiments/calibration")
    output_dir = Path("/Users/dro/rice/nfl-analytics/analysis/phase1_summary")
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("PHASE 1 CALIBRATION ANALYSIS")
    print("=" * 80)

    # Initialize analyzer
    analyzer = Phase1CalibrationAnalyzer(results_dir)

    # Print summary
    df = analyzer.print_summary()

    # Save CSV
    csv_path = output_dir / "phase1_summary.csv"
    df.to_csv(csv_path, index=False)
    print(f"\n‚úì Summary CSV saved to: {csv_path}")

    # Create visualization
    viz_path = output_dir / "phase1_calibration_analysis.png"
    analyzer.create_visualization(viz_path)

    # Save LaTeX table
    latex_path = Path(
        "/Users/dro/rice/nfl-analytics/analysis/dissertation/figures/out/phase1_prior_sensitivity_table.tex"
    )
    analyzer.save_latex_table(latex_path)

    print("\n" + "=" * 80)
    print("‚úì PHASE 1 ANALYSIS COMPLETE")
    print("=" * 80)
    print("\nOutputs:")
    print(f"  ‚Ä¢ Summary: {csv_path}")
    print(f"  ‚Ä¢ Visualization: {viz_path}")
    print(f"  ‚Ä¢ LaTeX table: {latex_path}")

    print("\nüìã NEXT STEPS:")
    print("  1. Wait for Phase 2.1 training to complete")
    print("  2. Run comparison: python py/analysis/compare_phase1_phase2.py")
    print("  3. Update dissertation with results")


if __name__ == "__main__":
    main()
