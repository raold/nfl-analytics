#!/usr/bin/env python3
from __future__ import annotations

"""
Core Statistical Testing Module.

Implements state-of-the-art statistical hypothesis testing methods for sports
betting analytics, including permutation tests, bootstrap methods, and
Bayesian approaches based on 2024 research.
"""

import logging
from abc import ABC, abstractmethod
from collections.abc import Callable
from dataclasses import dataclass
from enum import Enum

import numpy as np
import scipy.stats as stats
from scipy.stats import permutation_test

logger = logging.getLogger(__name__)


class TestType(Enum):
    """Supported statistical test types."""

    TWO_SAMPLE = "two_sample"
    ONE_SAMPLE = "one_sample"
    PAIRED = "paired"
    CORRELATION = "correlation"
    INDEPENDENCE = "independence"


@dataclass
class StatisticalResult:
    """Container for statistical test results."""

    test_name: str
    statistic: float
    p_value: float
    effect_size: float | None = None
    confidence_interval: tuple[float, float] | None = None
    method: str = "classical"
    n_permutations: int | None = None
    n_bootstrap: int | None = None
    interpretation: str = ""

    def is_significant(self, alpha: float = 0.05) -> bool:
        """Return True if result is statistically significant at level alpha."""
        return self.p_value < alpha

    def __str__(self) -> str:
        """String representation of results."""
        sig_str = "significant" if self.is_significant() else "not significant"
        result = f"{self.test_name}: statistic={self.statistic:.4f}, "
        result += f"p={self.p_value:.4f} ({sig_str})"

        if self.effect_size is not None:
            result += f", effect_size={self.effect_size:.4f}"

        if self.confidence_interval is not None:
            ci_low, ci_high = self.confidence_interval
            result += f", 95% CI=({ci_low:.4f}, {ci_high:.4f})"

        return result


class BaseStatisticalTest(ABC):
    """Abstract base class for statistical tests."""

    @abstractmethod
    def test(self, *args, **kwargs) -> StatisticalResult:
        """Perform the statistical test."""
        pass


class PermutationTest(BaseStatisticalTest):
    """
    Permutation-based hypothesis testing.

    Distribution-free tests that work by comparing observed test statistics
    to the distribution under the null hypothesis generated by permutations.
    Considered gold standard for non-parametric testing in 2024 research.
    """

    def __init__(self, n_permutations: int = 10000, random_state: int = 42):
        """
        Initialize permutation test.

        Args:
            n_permutations: Number of permutations for test
            random_state: Random seed for reproducibility
        """
        self.n_permutations = n_permutations
        self.random_state = random_state

    def two_sample_test(
        self,
        sample1: np.ndarray,
        sample2: np.ndarray,
        statistic_func: Callable | None = None,
        alternative: str = "two-sided",
    ) -> StatisticalResult:
        """
        Two-sample permutation test.

        Args:
            sample1: First sample
            sample2: Second sample
            statistic_func: Custom statistic function (default: difference in means)
            alternative: Type of test ("two-sided", "less", "greater")
        """
        if statistic_func is None:

            def statistic_func(x, y):
                return np.mean(x) - np.mean(y)

        def combined_statistic(x, axis):
            # Split combined data back into two samples
            n1 = len(sample1)
            return statistic_func(x[:n1], x[n1:])

        # Combine samples for permutation
        combined = np.concatenate([sample1, sample2])

        # Perform permutation test
        result = permutation_test(
            (combined,),
            combined_statistic,
            n_resamples=self.n_permutations,
            alternative=alternative,
            random_state=self.random_state,
        )

        observed_stat = statistic_func(sample1, sample2)

        # Calculate effect size (standardized mean difference)
        pooled_std = np.sqrt(
            (
                (len(sample1) - 1) * np.var(sample1, ddof=1)
                + (len(sample2) - 1) * np.var(sample2, ddof=1)
            )
            / (len(sample1) + len(sample2) - 2)
        )
        effect_size = observed_stat / pooled_std if pooled_std > 0 else 0

        return StatisticalResult(
            test_name="Two-sample Permutation Test",
            statistic=observed_stat,
            p_value=result.pvalue,
            effect_size=effect_size,
            method="permutation",
            n_permutations=self.n_permutations,
            interpretation=self._interpret_two_sample(result.pvalue, effect_size),
        )

    def one_sample_test(
        self, sample: np.ndarray, null_value: float = 0, alternative: str = "two-sided"
    ) -> StatisticalResult:
        """
        One-sample permutation test against null value.

        Args:
            sample: Sample data
            null_value: Null hypothesis value
            alternative: Type of test
        """
        # Center sample around null value
        centered_sample = sample - null_value

        def statistic_func(x, axis):
            return np.mean(x, axis=axis)

        # Use sign-flipping for one-sample test
        result = permutation_test(
            (centered_sample,),
            statistic_func,
            n_resamples=self.n_permutations,
            alternative=alternative,
            random_state=self.random_state,
        )

        observed_stat = np.mean(sample)
        effect_size = (observed_stat - null_value) / np.std(sample, ddof=1)

        return StatisticalResult(
            test_name="One-sample Permutation Test",
            statistic=observed_stat,
            p_value=result.pvalue,
            effect_size=effect_size,
            method="permutation",
            n_permutations=self.n_permutations,
            interpretation=self._interpret_one_sample(result.pvalue, effect_size),
        )

    def paired_test(
        self, before: np.ndarray, after: np.ndarray, alternative: str = "two-sided"
    ) -> StatisticalResult:
        """
        Paired permutation test.

        Args:
            before: Before measurements
            after: After measurements
            alternative: Type of test
        """
        differences = after - before
        return self.one_sample_test(differences, null_value=0, alternative=alternative)

    def correlation_test(
        self, x: np.ndarray, y: np.ndarray, method: str = "pearson"
    ) -> StatisticalResult:
        """
        Permutation test for correlation.

        Args:
            x: First variable
            y: Second variable
            method: Correlation method ("pearson", "spearman")
        """
        if method == "pearson":

            def correlation_func(a, b):
                return stats.pearsonr(a, b)[0]

        elif method == "spearman":

            def correlation_func(a, b):
                return stats.spearmanr(a, b)[0]

        else:
            raise ValueError("Method must be 'pearson' or 'spearman'")

        def statistic_func(y_perm, axis):
            return correlation_func(x, y_perm)

        result = permutation_test(
            (y,),
            statistic_func,
            n_resamples=self.n_permutations,
            alternative="two-sided",
            random_state=self.random_state,
        )

        observed_corr = correlation_func(x, y)

        return StatisticalResult(
            test_name=f"{method.title()} Correlation Permutation Test",
            statistic=observed_corr,
            p_value=result.pvalue,
            effect_size=observed_corr,  # Correlation is its own effect size
            method="permutation",
            n_permutations=self.n_permutations,
            interpretation=self._interpret_correlation(result.pvalue, observed_corr),
        )

    def _interpret_two_sample(self, p_value: float, effect_size: float) -> str:
        """Interpret two-sample test results."""
        sig_str = "significant" if p_value < 0.05 else "not significant"

        if abs(effect_size) < 0.2:
            size_str = "negligible"
        elif abs(effect_size) < 0.5:
            size_str = "small"
        elif abs(effect_size) < 0.8:
            size_str = "medium"
        else:
            size_str = "large"

        direction = "higher" if effect_size > 0 else "lower"

        return (
            f"Difference is {sig_str} (p={p_value:.4f}) with {size_str} "
            f"effect size ({effect_size:.3f}). Group 1 is {direction} than Group 2."
        )

    def _interpret_one_sample(self, p_value: float, effect_size: float) -> str:
        """Interpret one-sample test results."""
        sig_str = "significantly" if p_value < 0.05 else "not significantly"
        direction = "above" if effect_size > 0 else "below"

        return (
            f"Sample mean is {sig_str} different from null value "
            f"(p={p_value:.4f}, Cohen's d={effect_size:.3f}). "
            f"Sample is {direction} the null value."
        )

    def _interpret_correlation(self, p_value: float, correlation: float) -> str:
        """Interpret correlation test results."""
        sig_str = "significant" if p_value < 0.05 else "not significant"

        if abs(correlation) < 0.1:
            strength = "negligible"
        elif abs(correlation) < 0.3:
            strength = "weak"
        elif abs(correlation) < 0.5:
            strength = "moderate"
        elif abs(correlation) < 0.7:
            strength = "strong"
        else:
            strength = "very strong"

        direction = "positive" if correlation > 0 else "negative"

        return (
            f"Correlation is {sig_str} (p={p_value:.4f}) with {strength} "
            f"{direction} association (r={correlation:.3f})."
        )


class BootstrapTest(BaseStatisticalTest):
    """
    Bootstrap-based hypothesis testing and confidence intervals.

    Uses resampling with replacement to estimate sampling distributions
    and construct confidence intervals. Particularly useful for
    non-standard statistics and complex estimators.
    """

    def __init__(
        self, n_bootstrap: int = 10000, confidence_level: float = 0.95, random_state: int = 42
    ):
        """
        Initialize bootstrap test.

        Args:
            n_bootstrap: Number of bootstrap samples
            confidence_level: Confidence level for intervals
            random_state: Random seed
        """
        self.n_bootstrap = n_bootstrap
        self.confidence_level = confidence_level
        self.random_state = random_state

    def confidence_interval(
        self, data: np.ndarray, statistic_func: Callable = np.mean, method: str = "percentile"
    ) -> StatisticalResult:
        """
        Bootstrap confidence interval for a statistic.

        Args:
            data: Sample data
            statistic_func: Function to compute statistic
            method: CI method ("percentile", "bca")
        """
        # Compute observed statistic
        observed_stat = statistic_func(data)

        # Bootstrap resampling
        bootstrap_stats = []
        rng = np.random.RandomState(self.random_state)

        for _ in range(self.n_bootstrap):
            bootstrap_sample = rng.choice(data, size=len(data), replace=True)
            bootstrap_stat = statistic_func(bootstrap_sample)
            bootstrap_stats.append(bootstrap_stat)

        bootstrap_stats = np.array(bootstrap_stats)

        # Calculate confidence interval
        alpha = 1 - self.confidence_level

        if method == "percentile":
            ci_lower = np.percentile(bootstrap_stats, 100 * alpha / 2)
            ci_upper = np.percentile(bootstrap_stats, 100 * (1 - alpha / 2))
        elif method == "bca":
            # Bias-corrected and accelerated (BCa) interval
            ci_lower, ci_upper = self._bca_interval(
                data, statistic_func, bootstrap_stats, observed_stat, alpha
            )
        else:
            raise ValueError("Method must be 'percentile' or 'bca'")

        return StatisticalResult(
            test_name=f"Bootstrap {int(self.confidence_level*100)}% Confidence Interval",
            statistic=observed_stat,
            p_value=np.nan,  # Not applicable for CI
            confidence_interval=(ci_lower, ci_upper),
            method=f"bootstrap_{method}",
            n_bootstrap=self.n_bootstrap,
            interpretation=self._interpret_ci(observed_stat, ci_lower, ci_upper),
        )

    def hypothesis_test(
        self,
        data: np.ndarray,
        null_value: float,
        statistic_func: Callable = np.mean,
        alternative: str = "two-sided",
    ) -> StatisticalResult:
        """
        Bootstrap hypothesis test.

        Args:
            data: Sample data
            null_value: Null hypothesis value
            statistic_func: Function to compute statistic
            alternative: Type of test
        """
        observed_stat = statistic_func(data)

        # Center data under null hypothesis
        centered_data = data - (statistic_func(data) - null_value)

        # Bootstrap under null
        null_distribution = []
        rng = np.random.RandomState(self.random_state)

        for _ in range(self.n_bootstrap):
            bootstrap_sample = rng.choice(centered_data, size=len(data), replace=True)
            bootstrap_stat = statistic_func(bootstrap_sample)
            null_distribution.append(bootstrap_stat)

        null_distribution = np.array(null_distribution)

        # Calculate p-value
        if alternative == "two-sided":
            p_value = np.mean(
                np.abs(null_distribution - null_value) >= np.abs(observed_stat - null_value)
            )
        elif alternative == "greater":
            p_value = np.mean(null_distribution >= observed_stat)
        elif alternative == "less":
            p_value = np.mean(null_distribution <= observed_stat)
        else:
            raise ValueError("Alternative must be 'two-sided', 'greater', or 'less'")

        # Effect size
        effect_size = (observed_stat - null_value) / np.std(data, ddof=1)

        return StatisticalResult(
            test_name="Bootstrap Hypothesis Test",
            statistic=observed_stat,
            p_value=p_value,
            effect_size=effect_size,
            method="bootstrap",
            n_bootstrap=self.n_bootstrap,
            interpretation=self._interpret_hypothesis(p_value, effect_size, alternative),
        )

    def two_sample_test(
        self, sample1: np.ndarray, sample2: np.ndarray, statistic_func: Callable | None = None
    ) -> StatisticalResult:
        """
        Bootstrap two-sample test.

        Args:
            sample1: First sample
            sample2: Second sample
            statistic_func: Function to compute difference statistic
        """
        if statistic_func is None:

            def statistic_func(x, y):
                return np.mean(x) - np.mean(y)

        observed_diff = statistic_func(sample1, sample2)

        # Pool samples under null hypothesis of no difference
        pooled = np.concatenate([sample1, sample2])
        n1, n2 = len(sample1), len(sample2)

        # Bootstrap under null
        null_distribution = []
        rng = np.random.RandomState(self.random_state)

        for _ in range(self.n_bootstrap):
            pooled_bootstrap = rng.choice(pooled, size=len(pooled), replace=True)
            boot_sample1 = pooled_bootstrap[:n1]
            boot_sample2 = pooled_bootstrap[n1 : n1 + n2]
            boot_diff = statistic_func(boot_sample1, boot_sample2)
            null_distribution.append(boot_diff)

        null_distribution = np.array(null_distribution)

        # Two-sided p-value
        p_value = np.mean(np.abs(null_distribution) >= np.abs(observed_diff))

        # Effect size (Cohen's d)
        pooled_std = np.sqrt(
            ((n1 - 1) * np.var(sample1, ddof=1) + (n2 - 1) * np.var(sample2, ddof=1))
            / (n1 + n2 - 2)
        )
        effect_size = observed_diff / pooled_std if pooled_std > 0 else 0

        return StatisticalResult(
            test_name="Bootstrap Two-sample Test",
            statistic=observed_diff,
            p_value=p_value,
            effect_size=effect_size,
            method="bootstrap",
            n_bootstrap=self.n_bootstrap,
            interpretation=self._interpret_two_sample_bootstrap(p_value, effect_size),
        )

    def _bca_interval(
        self,
        data: np.ndarray,
        statistic_func: Callable,
        bootstrap_stats: np.ndarray,
        observed_stat: float,
        alpha: float,
    ) -> tuple[float, float]:
        """Compute bias-corrected and accelerated (BCa) confidence interval."""
        # Bias correction
        n_below = np.sum(bootstrap_stats < observed_stat)
        bias_correction = stats.norm.ppf(n_below / self.n_bootstrap)

        # Acceleration (jackknife)
        n = len(data)
        jackknife_stats = []
        for i in range(n):
            jackknife_sample = np.concatenate([data[:i], data[i + 1 :]])
            jackknife_stat = statistic_func(jackknife_sample)
            jackknife_stats.append(jackknife_stat)

        jackknife_stats = np.array(jackknife_stats)
        jackknife_mean = np.mean(jackknife_stats)

        numerator = np.sum((jackknife_mean - jackknife_stats) ** 3)
        denominator = 6 * (np.sum((jackknife_mean - jackknife_stats) ** 2)) ** 1.5
        acceleration = numerator / denominator if denominator != 0 else 0

        # Adjusted quantiles
        z_alpha_2 = stats.norm.ppf(alpha / 2)
        z_1_alpha_2 = stats.norm.ppf(1 - alpha / 2)

        alpha1 = stats.norm.cdf(
            bias_correction
            + (bias_correction + z_alpha_2) / (1 - acceleration * (bias_correction + z_alpha_2))
        )
        alpha2 = stats.norm.cdf(
            bias_correction
            + (bias_correction + z_1_alpha_2) / (1 - acceleration * (bias_correction + z_1_alpha_2))
        )

        ci_lower = np.percentile(bootstrap_stats, 100 * alpha1)
        ci_upper = np.percentile(bootstrap_stats, 100 * alpha2)

        return ci_lower, ci_upper

    def _interpret_ci(self, statistic: float, ci_lower: float, ci_upper: float) -> str:
        """Interpret confidence interval."""
        return (
            f"The {int(self.confidence_level*100)}% confidence interval for "
            f"the statistic is ({ci_lower:.4f}, {ci_upper:.4f}). "
            f"The observed statistic is {statistic:.4f}."
        )

    def _interpret_hypothesis(self, p_value: float, effect_size: float, alternative: str) -> str:
        """Interpret bootstrap hypothesis test."""
        sig_str = "significant" if p_value < 0.05 else "not significant"
        return (
            f"Bootstrap test result is {sig_str} (p={p_value:.4f}) "
            f"with effect size {effect_size:.3f} using {alternative} alternative."
        )

    def _interpret_two_sample_bootstrap(self, p_value: float, effect_size: float) -> str:
        """Interpret bootstrap two-sample test."""
        sig_str = "significant" if p_value < 0.05 else "not significant"
        direction = "higher" if effect_size > 0 else "lower"

        return (
            f"Bootstrap two-sample test shows {sig_str} difference "
            f"(p={p_value:.4f}, Cohen's d={effect_size:.3f}). "
            f"Sample 1 is {direction} than Sample 2."
        )


class BayesianTest(BaseStatisticalTest):
    """
    Bayesian hypothesis testing methods.

    Provides Bayesian alternatives to frequentist testing, including
    Bayes factors and credible intervals.
    """

    def __init__(self, prior_params: dict[str, float] | None = None):
        """
        Initialize Bayesian test.

        Args:
            prior_params: Parameters for prior distributions
        """
        self.prior_params = prior_params or {}

    def bayes_factor_t_test(
        self, sample1: np.ndarray, sample2: np.ndarray, prior_scale: float = 1.0
    ) -> StatisticalResult:
        """
        Bayesian t-test using Bayes factor.

        Args:
            sample1: First sample
            sample2: Second sample
            prior_scale: Scale parameter for Cauchy prior on effect size
        """
        # This is a simplified implementation
        # Full implementation would use specialized libraries like PyMC or Stan

        n1, n2 = len(sample1), len(sample2)
        m1, m2 = np.mean(sample1), np.mean(sample2)
        s1, s2 = np.std(sample1, ddof=1), np.std(sample2, ddof=1)

        # Pooled standard error
        se_pooled = np.sqrt(s1**2 / n1 + s2**2 / n2)

        # Effect size
        effect_size = (m1 - m2) / se_pooled if se_pooled > 0 else 0

        # Approximate Bayes factor (simplified)
        # This is a rough approximation - real implementation would be more complex
        t_stat = effect_size
        df = n1 + n2 - 2

        # Using BIC approximation for Bayes factor
        classical_p = 2 * (1 - stats.t.cdf(abs(t_stat), df))
        log_bf10 = -0.5 * np.log(n1 + n2) - 0.5 * t_stat**2
        bf10 = np.exp(log_bf10)

        # Interpret Bayes factor
        if bf10 > 10:
            evidence = "strong evidence for H1"
        elif bf10 > 3:
            evidence = "moderate evidence for H1"
        elif bf10 > 1:
            evidence = "weak evidence for H1"
        elif bf10 > 0.33:
            evidence = "weak evidence for H0"
        elif bf10 > 0.1:
            evidence = "moderate evidence for H0"
        else:
            evidence = "strong evidence for H0"

        return StatisticalResult(
            test_name="Bayesian t-test",
            statistic=bf10,
            p_value=classical_p,  # For comparison
            effect_size=effect_size,
            method="bayesian",
            interpretation=f"Bayes factor BF10 = {bf10:.3f}, {evidence}",
        )

    def credible_interval(
        self,
        data: np.ndarray,
        credibility: float = 0.95,
        prior_mean: float = 0,
        prior_precision: float = 1,
    ) -> StatisticalResult:
        """
        Bayesian credible interval for mean with normal prior.

        Args:
            data: Sample data
            credibility: Credibility level
            prior_mean: Prior mean
            prior_precision: Prior precision (1/variance)
        """
        n = len(data)
        sample_mean = np.mean(data)
        sample_precision = n / np.var(data, ddof=1) if np.var(data, ddof=1) > 0 else 1

        # Posterior parameters (conjugate prior)
        posterior_precision = prior_precision + sample_precision
        posterior_mean = (
            prior_precision * prior_mean + sample_precision * sample_mean
        ) / posterior_precision
        posterior_variance = 1 / posterior_precision

        # Credible interval
        alpha = 1 - credibility
        z_alpha_2 = stats.norm.ppf(1 - alpha / 2)

        ci_lower = posterior_mean - z_alpha_2 * np.sqrt(posterior_variance)
        ci_upper = posterior_mean + z_alpha_2 * np.sqrt(posterior_variance)

        return StatisticalResult(
            test_name=f"Bayesian {int(credibility*100)}% Credible Interval",
            statistic=posterior_mean,
            p_value=np.nan,
            confidence_interval=(ci_lower, ci_upper),
            method="bayesian",
            interpretation=(
                f"The {int(credibility*100)}% credible interval for "
                f"the mean is ({ci_lower:.4f}, {ci_upper:.4f}). "
                f"Posterior mean is {posterior_mean:.4f}."
            ),
        )


class ClassicalTests(BaseStatisticalTest):
    """
    Classical statistical tests for compatibility.

    Provides standard parametric and non-parametric tests
    for comparison with modern methods.
    """

    def t_test(
        self,
        sample1: np.ndarray,
        sample2: np.ndarray | None = None,
        null_value: float = 0,
        alternative: str = "two-sided",
        equal_var: bool = True,
    ) -> StatisticalResult:
        """
        Student's t-test or Welch's t-test.

        Args:
            sample1: First sample (or only sample for one-sample test)
            sample2: Second sample (None for one-sample test)
            null_value: Null hypothesis value
            alternative: Type of test
            equal_var: Assume equal variances
        """
        if sample2 is None:
            # One-sample t-test
            statistic, p_value = stats.ttest_1samp(sample1, null_value, alternative=alternative)
            effect_size = (np.mean(sample1) - null_value) / np.std(sample1, ddof=1)
            test_name = "One-sample t-test"
        else:
            # Two-sample t-test
            statistic, p_value = stats.ttest_ind(
                sample1, sample2, alternative=alternative, equal_var=equal_var
            )

            # Cohen's d
            if equal_var:
                pooled_std = np.sqrt(
                    (
                        (len(sample1) - 1) * np.var(sample1, ddof=1)
                        + (len(sample2) - 1) * np.var(sample2, ddof=1)
                    )
                    / (len(sample1) + len(sample2) - 2)
                )
            else:
                pooled_std = np.sqrt((np.var(sample1, ddof=1) + np.var(sample2, ddof=1)) / 2)

            effect_size = (np.mean(sample1) - np.mean(sample2)) / pooled_std
            test_name = "Two-sample t-test" if equal_var else "Welch's t-test"

        return StatisticalResult(
            test_name=test_name,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            method="classical",
            interpretation=self._interpret_t_test(p_value, effect_size, test_name),
        )

    def mann_whitney_u(
        self, sample1: np.ndarray, sample2: np.ndarray, alternative: str = "two-sided"
    ) -> StatisticalResult:
        """
        Mann-Whitney U test (Wilcoxon rank-sum test).

        Args:
            sample1: First sample
            sample2: Second sample
            alternative: Type of test
        """
        statistic, p_value = stats.mannwhitneyu(sample1, sample2, alternative=alternative)

        # Effect size: rank-biserial correlation
        n1, n2 = len(sample1), len(sample2)
        r = 1 - (2 * statistic) / (n1 * n2)

        return StatisticalResult(
            test_name="Mann-Whitney U test",
            statistic=statistic,
            p_value=p_value,
            effect_size=r,
            method="classical",
            interpretation=self._interpret_mann_whitney(p_value, r),
        )

    def wilcoxon_signed_rank(
        self, sample: np.ndarray, null_value: float = 0, alternative: str = "two-sided"
    ) -> StatisticalResult:
        """
        Wilcoxon signed-rank test.

        Args:
            sample: Sample data (or differences for paired test)
            null_value: Null hypothesis value
            alternative: Type of test
        """
        differences = sample - null_value
        statistic, p_value = stats.wilcoxon(differences, alternative=alternative)

        # Effect size: r = Z / sqrt(N)
        n = len(differences[differences != 0])  # Exclude ties
        z_score = (
            stats.norm.ppf(1 - p_value / 2)
            if alternative == "two-sided"
            else stats.norm.ppf(1 - p_value)
        )
        effect_size = z_score / np.sqrt(n) if n > 0 else 0

        return StatisticalResult(
            test_name="Wilcoxon signed-rank test",
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            method="classical",
            interpretation=self._interpret_wilcoxon(p_value, effect_size),
        )

    def chi_square_test(
        self, observed: np.ndarray, expected: np.ndarray | None = None
    ) -> StatisticalResult:
        """
        Chi-square goodness-of-fit or independence test.

        Args:
            observed: Observed frequencies
            expected: Expected frequencies (None for independence test)
        """
        if expected is None:
            # Chi-square test of independence
            statistic, p_value, dof, expected_calc = stats.chi2_contingency(observed)
            test_name = "Chi-square test of independence"

            # Cramér's V effect size
            n = np.sum(observed)
            cramer_v = np.sqrt(statistic / (n * (min(observed.shape) - 1)))
            effect_size = cramer_v
        else:
            # Chi-square goodness-of-fit
            statistic, p_value = stats.chisquare(observed, expected)
            test_name = "Chi-square goodness-of-fit test"
            effect_size = None  # Not standard for goodness-of-fit

        return StatisticalResult(
            test_name=test_name,
            statistic=statistic,
            p_value=p_value,
            effect_size=effect_size,
            method="classical",
            interpretation=self._interpret_chi_square(p_value, effect_size, test_name),
        )

    def _interpret_t_test(self, p_value: float, effect_size: float, test_name: str) -> str:
        """Interpret t-test results."""
        sig_str = "significant" if p_value < 0.05 else "not significant"
        return (
            f"{test_name} result is {sig_str} (p={p_value:.4f}) "
            f"with Cohen's d = {effect_size:.3f}"
        )

    def _interpret_mann_whitney(self, p_value: float, effect_size: float) -> str:
        """Interpret Mann-Whitney U test results."""
        sig_str = "significant" if p_value < 0.05 else "not significant"
        return (
            f"Mann-Whitney U test result is {sig_str} (p={p_value:.4f}) "
            f"with rank-biserial correlation r = {effect_size:.3f}"
        )

    def _interpret_wilcoxon(self, p_value: float, effect_size: float) -> str:
        """Interpret Wilcoxon signed-rank test results."""
        sig_str = "significant" if p_value < 0.05 else "not significant"
        return (
            f"Wilcoxon signed-rank test result is {sig_str} (p={p_value:.4f}) "
            f"with effect size r = {effect_size:.3f}"
        )

    def _interpret_chi_square(
        self, p_value: float, effect_size: float | None, test_name: str
    ) -> str:
        """Interpret chi-square test results."""
        sig_str = "significant" if p_value < 0.05 else "not significant"
        result = f"{test_name} result is {sig_str} (p={p_value:.4f})"

        if effect_size is not None:
            result += f" with Cramér's V = {effect_size:.3f}"

        return result


def test():
    """Test the statistical testing module."""
    np.random.seed(42)

    # Generate test data
    sample1 = np.random.normal(0, 1, 100)
    sample2 = np.random.normal(0.5, 1, 100)

    print("=== Statistical Testing Module Demo ===\n")

    # Permutation test
    perm_test = PermutationTest(n_permutations=5000)
    result = perm_test.two_sample_test(sample1, sample2)
    print("Permutation Test:")
    print(result)
    print()

    # Bootstrap test
    boot_test = BootstrapTest(n_bootstrap=5000)
    result = boot_test.two_sample_test(sample1, sample2)
    print("Bootstrap Test:")
    print(result)
    print()

    # Classical t-test
    classical = ClassicalTests()
    result = classical.t_test(sample1, sample2)
    print("Classical t-test:")
    print(result)
    print()

    # Bayesian test
    bayes_test = BayesianTest()
    result = bayes_test.bayes_factor_t_test(sample1, sample2)
    print("Bayesian Test:")
    print(result)


if __name__ == "__main__":
    test()
