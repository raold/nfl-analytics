\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Gaussian copula joint exceedance}}{17}{figure.caption.120}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Reliability diagram (95\% CIs)}}{18}{figure.caption.128}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Integer‑margin pmf comparison}}{23}{figure.caption.159}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Permutation feature importance for XGBoost baseline ensemble on 2024 holdout (281 games). Market delta and spread dominate (0.24 and 0.18 normalized importance), confirming market efficiency as the primary signal. Rolling EPA metrics and QB status contribute moderately, while weather and travel have minimal impact.}}{38}{figure.caption.250}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Relative weight by season under exponential decay with half‑life $H\in \{3,4,5\}$ (centered on 2024). Annotations highlight 1999 and 2024. Figure generated by \texttt {notebooks/00\_timeframe\_ablation.qmd}.}}{39}{figure.caption.255}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Rolling OOS log loss}}{44}{figure.caption.273}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Rolling OOS ECE}}{45}{figure.caption.274}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Reliability curves (2024 holdout)}}{46}{figure.caption.275}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Integer‑margin frequencies (holdout)}}{57}{figure.caption.336}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Baseline calibration}}{62}{figure.caption.370}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Copula impact on teaser/SGP EV}}{63}{figure.caption.383}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Per-season reliability: GLM baseline (2015--2019)}}{70}{figure.caption.428}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Per-season reliability: GLM baseline (2020--2024)}}{71}{figure.caption.429}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces GLM reliability (Platt)}}{72}{figure.caption.430}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Model evolution performance}}{76}{figure.caption.455}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces Ensemble component performance}}{77}{figure.caption.460}%
\contentsline {figure}{\numberline {4.12}{\ignorespaces Calibration comparison}}{78}{figure.caption.461}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Offline RL learning curves (median and IQR across seeds).}}{96}{figure.caption.516}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Sensitivity of EV and calibration to key hyperparameters (entropy scale, target smoothing, clip).}}{97}{figure.caption.517}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces End-to-end offline RL workflow from data to promotion.}}{98}{figure.caption.525}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Sensitivity of growth/drawdown to pessimism level $\alpha $ in Kelly‑LCB and the RL policy’s calibration to $p$ uncertainty.}}{101}{figure.caption.546}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Final bankroll distribution}}{109}{figure.caption.585}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces Fractional Kelly bankroll trajectories}}{110}{figure.caption.589}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Simulated teaser expected value surface as a function of leg success probabilities. The zero contour (white) marks the middle threshold that informs acceptance tests inside the simulator (\Cref {sec:teaser-math}).}}{115}{figure.caption.610}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces Acceptance pass/fail rates by season and test category (margins, key masses, dependence).}}{122}{figure.caption.660}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces Relationship between acceptance outcomes and live performance (e.g., CLV/ROI). Failing acceptance correlates with degraded live metrics, justifying the gate.}}{122}{figure.caption.661}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces Comparison of 90\% credible interval coverage across all methods investigated. The horizontal dashed line indicates the target coverage of 90\%. All single-output BNN variants (baseline, with Vegas features, with opponent features, with optimized hyperparameters) remain severely under-calibrated regardless of modifications. In contrast, non-Bayesian baselines (conformal prediction, quantile regression) and the architectural innovation (multi-output BNN) achieve calibration near or exceeding the target.}}{145}{figure.caption.748}%
\contentsline {figure}{\numberline {8.2}{\ignorespaces Incremental impact of feature engineering on 90\% credible interval coverage. Each bar represents a cumulative feature configuration: baseline (4 features), +Vegas lines (6 features), +environment (10 features), +opponent defense (9 features, environment features removed). Error bars represent bootstrapped 95\% confidence intervals (1000 resamples). The horizontal red dashed line indicates the target 90\% coverage. Despite incorporating domain expertise and multiple information sources, the best feature set improves coverage by only 5.1 percentage points, leaving the model severely under-calibrated.}}{149}{figure.caption.757}%
\contentsline {figure}{\numberline {8.3}{\ignorespaces Coverage-sharpness trade-off across uncertainty quantification methods. Each point represents a method, with x-axis showing average 90\% interval width (lower is sharper) and y-axis showing empirical coverage (closer to 90\% is better calibrated). The dashed horizontal line indicates target 90\% coverage; the shaded region marks acceptable calibration (85-95\%). The dashed vertical line shows the single-output BNN's interval width. Single-output BNN variants cluster in the bottom-left (narrow but under-calibrated), while quantile regression and conformal prediction achieve calibration by drastically widening intervals. The multi-output BNN achieves near-optimal balance: strong calibration without extreme interval widening.}}{158}{figure.caption.802}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces Claude AI semantic research workflow. User queries in natural language are translated to executable code (SQL, R, Python), validated with statistical rigor, and interpreted with domain knowledge. The conversational loop enables iterative refinement.}}{174}{figure.caption.874}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {12.1}{\ignorespaces Profit Optimization System Architecture}}{223}{figure.caption.1112}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {D.1}{\ignorespaces CQL Training Curves: Total loss, TD error, CQL penalty, and mean Q-value across 2000 epochs. Training completed in approximately 9 minutes on NVIDIA RTX 4090 (24GB VRAM) with CUDA acceleration. Loss reduction of 75\% demonstrates effective convergence with conservative regularization.}}{243}{figure.caption.1238}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
