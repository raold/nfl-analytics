\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Gaussian copula joint exceedance}}{17}{figure.caption.230}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Reliability diagram (95\% CIs)}}{19}{figure.caption.240}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Integer‑margin pmf comparison}}{24}{figure.caption.271}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Feature-importance snapshot (permutation) for a baseline ensemble; higher is more important.}}{38}{figure.caption.321}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Relative weight by season under exponential decay with half‑life $H\in \{3,4,5\}$ (centered on 2024). Annotations highlight 1999 and 2024. Figure generated by \texttt {notebooks/00\_timeframe\_ablation.qmd}.}}{40}{figure.caption.328}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Rolling OOS log loss}}{43}{figure.caption.344}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces Rolling OOS ECE}}{44}{figure.caption.345}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Reliability curves (2024 holdout)}}{45}{figure.caption.346}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Integer‑margin frequencies (holdout)}}{55}{figure.caption.401}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces Baseline calibration}}{60}{figure.caption.435}%
\contentsline {figure}{\numberline {6.6}{\ignorespaces Copula impact on teaser/SGP EV}}{61}{figure.caption.448}%
\contentsline {figure}{\numberline {6.7}{\ignorespaces Per-season reliability: GLM baseline (2015--2019)}}{65}{figure.caption.474}%
\contentsline {figure}{\numberline {6.8}{\ignorespaces Per-season reliability: GLM baseline (2020--2024)}}{66}{figure.caption.475}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Offline RL learning curves (median and IQR across seeds).}}{80}{figure.caption.516}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces Sensitivity of EV and calibration to key hyperparameters (entropy scale, target smoothing, clip).}}{81}{figure.caption.517}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces End-to-end offline RL workflow from data to promotion.}}{82}{figure.caption.525}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {8.1}{\ignorespaces Final bankroll distribution}}{92}{figure.caption.582}%
\contentsline {figure}{\numberline {8.2}{\ignorespaces Fractional Kelly bankroll trajectories}}{93}{figure.caption.588}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces Simulated teaser expected value surface as a function of leg success probabilities. The zero contour (white) marks the middle threshold that informs acceptance tests inside the simulator (\Cref {sec:teaser-math}).}}{97}{figure.caption.603}%
\contentsline {figure}{\numberline {9.2}{\ignorespaces Acceptance pass/fail rates by season and test category (margins, key masses, dependence).}}{104}{figure.caption.654}%
\contentsline {figure}{\numberline {9.3}{\ignorespaces Relationship between acceptance outcomes and live performance (e.g., CLV/ROI). Failing acceptance correlates with degraded live metrics, justifying the gate.}}{105}{figure.caption.655}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {11.1}{\ignorespaces Claude AI semantic research workflow. User queries in natural language are translated to executable code (SQL, R, Python), validated with statistical rigor, and interpreted with domain knowledge. The conversational loop enables iterative refinement.}}{129}{figure.caption.741}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {13.1}{\ignorespaces Profit Optimization System Architecture}}{173}{figure.caption.954}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces CQL Training Curves: Total loss, TD error, CQL penalty, and mean Q-value across 2000 epochs. Training completed in approximately 9 minutes on NVIDIA RTX 4090 (24GB VRAM) with CUDA acceleration. Loss reduction of 75\% demonstrates effective convergence with conservative regularization.}}{187}{figure.caption.1013}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
