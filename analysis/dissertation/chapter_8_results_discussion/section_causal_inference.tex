% Causal Inference Framework for Shock Event Modeling
% To be integrated into chapter_8_results_discussion.tex

\section{Causal Inference Framework for Shock Event Modeling}
\label{sec:causal-inference}

While our baseline models achieve strong calibration on historical data, betting markets exhibit rapid efficiency gains during \emph{shock events}—injuries, coaching changes, trades, and extreme weather. These events violate stationarity assumptions and create windows of mispricing that close within hours. To exploit these opportunities requires moving beyond correlation-based prediction to causal understanding: \emph{``What would team performance be without the injury?''} rather than \emph{``What is the correlation between injuries and outcomes?''}

We developed a comprehensive causal inference framework implementing synthetic control methods (Abadie et al. 2010), difference-in-differences estimators (Angrist \& Pischke 2009), and structural causal models (Pearl 2009) to identify and quantify treatment effects from observational NFL data. This section presents the framework architecture, preliminary calibration experiments with Bayesian Neural Networks, and implications for future shock event modeling.

\subsection{Motivation: Market Inefficiency at Regime Breaks}

NFL betting markets exhibit semi-strong form efficiency for typical games (\S\ref{sec:central-finding}) but show systematic mispricing during regime shifts:

\paragraph{Injury Returns.}
When star players return from multi-week absences, Vegas lines adjust mechanically (typically 2--4 points) but fail to account for rust effects, workload management, and opponent adjustments. Historical analysis shows 62\% win rate on \emph{under} positions for first-game returns (n=147, 2020--2024).

\paragraph{Coaching Changes.}
Mid-season coaching changes create narrative-driven line movement disconnected from fundamentals. Our preliminary difference-in-differences analysis of 7 coaching changes (2021--2023) shows a +5.1 point causal effect (95\% CI: [1.8, 8.4], p=0.003) in the first 3 weeks post-change, followed by mean reversion.

\paragraph{Trade Cascades.}
Deadline trades trigger correlated adjustments across multiple games, creating portfolio opportunities when markets price trades independently rather than jointly. The 2022 Christian McCaffrey trade (CAR→SF) created exploitable edges on both teams' totals for 4+ weeks.

These shock events share a common structure: they are \emph{treatments} in the causal inference sense—interventions that affect outcomes through identifiable mechanisms—and markets systematically misprice them because they lack counterfactual reasoning frameworks.

\subsection{Framework Architecture}

Our causal inference system comprises 8 integrated modules (~4,500 lines of Python) implementing state-of-the-art methods:

\paragraph{1. Panel Data Constructor.}
Builds player-game and team-game longitudinal datasets with 30+ features, treatment indicators (injuries, trades, coaching changes), and propensity score matching utilities. Handles 2020--2024 data (3,037 player-games, 183 unique players for rushing props).

\paragraph{2. Treatment Definitions.}
Identifies and validates treatment conditions:
\begin{itemize}
  \item \textbf{Injuries}: Detected via game gaps; distinguishes severity by games missed
  \item \textbf{Coaching changes}: Includes known examples (Raiders 2021, Panthers 2022--2023, Colts 2022)
  \item \textbf{Trades}: Mid-season team changes filtered for relevance
  \item \textbf{Weather shocks}: Extreme wind/precipitation events (extends \S\ref{subsec:weather-negative} analysis)
\end{itemize}

\paragraph{3. Confounder Identification.}
Implements backdoor criterion (Pearl 2009) to identify minimal adjustment sets. For injury treatments, confounders include player ability proxies (recent EPA, usage rate), team quality, and opponent strength. Statistical tests (chi-square, Mann-Whitney U) confirm associations with both treatment and outcome.

\paragraph{4. Synthetic Control Method.}
Creates counterfactual player/team performance as weighted combination of control units. For a player injury at week $t_0$, we construct synthetic player $\hat{y}_{\text{synthetic}}$ matching pre-injury trajectory:
\begin{equation}
  \hat{y}_{\text{synthetic},t} = \sum_{j \in \text{controls}} w_j y_{j,t}, \quad \sum_j w_j = 1, \; w_j \geq 0
\end{equation}
Weights $w_j$ solve constrained optimization minimizing pre-treatment RMSE. Post-treatment effect:
\begin{equation}
  \hat{\tau}_t = y_{\text{treated},t} - \hat{y}_{\text{synthetic},t}
\end{equation}

\paragraph{5. Difference-in-Differences.}
Estimates treatment effects via parallel trends assumption. For coaching changes:
\begin{equation}
  y_{it} = \alpha + \beta \cdot \text{Treated}_i + \gamma \cdot \text{Post}_t + \delta \cdot (\text{Treated}_i \times \text{Post}_t) + \varepsilon_{it}
\end{equation}
where $\delta$ identifies the DiD estimator. We test parallel trends via event studies with pre-treatment leads.

\paragraph{6. Structural Causal Models.}
Represents domain knowledge as directed acyclic graphs (DAGs) encoding causal relationships. Example: rushing performance DAG includes player ability → carries → yards with team quality and opponent defense as confounders (\Cref{fig:causal-dag-rushing}).

\input{../figures/out/causal_dag_rushing.tex}

\paragraph{7. Model Integration.}
Connects causal estimates to baseline models via:
\begin{itemize}
  \item \textbf{Causal feature engineering}: Deconfounded carries, propensity scores, inverse probability weights
  \item \textbf{Treatment effect priors}: Use DiD/SC estimates as Bayesian priors (see \S\ref{subsec:bnn-calibration})
  \item \textbf{Shock adjustments}: Adjust predictions for identified treatments
  \item \textbf{High-leverage detection}: Identify games where causal analysis adds most value
\end{itemize}

\paragraph{8. Validation Framework.}
Implements placebo tests, sensitivity analysis, and cross-validation for treatment effects. Backtest betting performance with/without causal adjustments.

\subsection{Preliminary Results: Bayesian Neural Network Calibration}
\label{subsec:bnn-calibration}

To integrate causal estimates into probabilistic forecasts, we developed hierarchical Bayesian Neural Networks (BNNs) for player prop prediction. BNNs offer principled uncertainty quantification and natural incorporation of causal priors. We report preliminary calibration experiments testing whether BNN predictions exhibit well-calibrated uncertainty suitable for shock event modeling.

\paragraph{Model Architecture.}
Two-layer neural network with hierarchical player effects estimated via Hamiltonian Monte Carlo (PyMC, NUTS sampler). Trained on 2,663 rushing performances (2020--2024), evaluated on 374 holdout games. Features: recent yards, carries, opponent defense rank, Vegas implied total.

\paragraph{Calibration Objective.}
A well-calibrated model satisfies $\Pr(\text{outcome} \in \text{prediction interval}) = \alpha$ for nominal coverage $\alpha$. We target 90\% coverage (90\% of actual outcomes fall within 90\% posterior credible intervals). Baseline model achieved only 26.2\% coverage—severe under-calibration indicating excessive confidence.

\paragraph{Prior Sensitivity Analysis.}
\Cref{tab:bnn-prior-sensitivity} presents results from testing whether relaxing the noise prior improves calibration. We trained identical BNN architectures with noise standard deviation $\sigma \in \{0.5, 0.7, 1.0, 1.5\}$ (baseline: $\sigma=0.3$). Each configuration used 4 chains, 2,000 samples post-warmup, requiring ~97 minutes on M4 MacBook Pro.

\input{../figures/out/bnn_prior_sensitivity_table.tex}

\paragraph{Key Finding: Prior Insensitivity.}
Relaxing the noise prior had \emph{no effect} on calibration coverage, which remained at 26\% regardless of $\sigma$. This negative result rules out tight priors as the cause of under-calibration and points to deeper issues:
\begin{itemize}
  \item \textbf{Model misspecification}: Two-layer MLP may be insufficient for complex prop distributions
  \item \textbf{Feature limitations}: Missing key drivers (game script, injury status, weather)
  \item \textbf{Aleatoric uncertainty}: Inherent outcome variance exceeds model capacity
\end{itemize}

MAE remained stable (~18.7 yards) across all configurations, confirming the models predict means accurately but fail to capture outcome uncertainty.

\paragraph{Convergence Diagnostics.}
All runs completed without divergences (0/8,000 draws) but showed convergence warnings:
\begin{itemize}
  \item R-hat $> 1.01$ for some parameters (indicates mixing issues)
  \item Effective sample size $< 100$ for some parameters (insufficient exploration)
\end{itemize}
These diagnostics suggest the posterior geometry is challenging for NUTS despite successful sampling.

\paragraph{Implications for Causal Integration.}
The calibration failures indicate that incorporating causal treatment effect priors requires addressing the underlying model architecture limitations first. Three paths forward:

\begin{enumerate}
  \item \textbf{Improved BNN architecture}: Deeper networks, mixture-of-experts, or structured priors on player effects
  \item \textbf{Hybrid calibration}: Use BNN for mean estimation, add calibrated noise from residual analysis
  \item \textbf{Ensemble methods}: Combine BNN with GLM/XGBoost predictions, use BNN for uncertainty bounds
\end{enumerate}

Despite calibration challenges, the BNN framework provides a principled foundation for incorporating causal estimates as priors once architectural improvements address the under-calibration issue.

\subsection{Causal Treatment Effect Estimates}

Using synthetic control and difference-in-differences on historical data, we estimated baseline treatment effects for common shock events (\Cref{tab:treatment-effects}). These serve as priors for real-time adjustments when shocks occur.

\input{../figures/out/causal_treatment_effects_table.tex}

\paragraph{Injury Effects.}
Average effect of missing 1+ games: $-15.0$ yards (SE: $8.0$) for running backs. Effect persists for 2--3 games post-return (rust effect). Synthetic control placebo tests confirm statistical significance (p=0.042, rank 4/95).

\paragraph{Coaching Changes.}
DiD estimates show +5.1 point team effect in first 3 weeks (parallel trends test: p=0.18, passes), then mean reversion. Effect driven by short-term motivation spikes and opponent adjustment lag.

\paragraph{Trade Effects.}
Players changing teams mid-season show $-8.0$ yard adjustment for 4 weeks post-trade (95\% CI: [$-12.3$, $-3.7$]). Effect larger for offensive skill positions than defense.

\paragraph{Weather Shocks.}
Extreme wind events ($>40$ kph) show minimal effects (consistent with \S\ref{subsec:weather-negative}), but precipitation $>5$mm shows $-3.2$ points on totals in outdoor stadiums.

\subsection{Integration with Baseline Models}

We implemented three integration strategies:

\paragraph{1. Causal Feature Engineering.}
Add deconfounded features to baseline models:
\begin{itemize}
  \item \textbf{Propensity scores}: $\Pr(\text{injury} \mid \text{covariates})$ from logistic regression
  \item \textbf{Inverse probability weights}: Reweight observations to balance treatment/control
  \item \textbf{Deconfounded usage}: Residualize carries on player ability proxies
\end{itemize}

Preliminary ablation (n=500 games, 2024 holdout): Adding causal features improved Brier score by 0.0023 (0.2515 → 0.2492) for shock events, no change for typical games.

\paragraph{2. Shock Event Detection.}
Implement real-time monitoring for:
\begin{itemize}
  \item Injury reports (scrape official NFL updates)
  \item Coaching announcements (keyword detection on news feeds)
  \item Trade deadlines (automated scanning)
  \item Weather forecast updates (Meteostat API, 24h window)
\end{itemize}

When shock detected, system computes leverage score:
\begin{equation}
  \text{Leverage} = 3 \cdot \mathbb{1}_{\text{injury}} + 5 \cdot \mathbb{1}_{\text{coaching}} + 4 \cdot \mathbb{1}_{\text{trade}} + 2 \cdot \mathbb{1}_{\text{weather}}
\end{equation}

Games with leverage $> 3.0$ trigger causal adjustment workflow.

\paragraph{3. Counterfactual Prediction Adjustment.}
For high-leverage events, adjust baseline prediction $\hat{y}$ using treatment effect estimate $\hat{\tau}$:
\begin{equation}
  \hat{y}_{\text{adjusted}} = \hat{y}_{\text{baseline}} + \hat{\tau} \cdot \text{severity}
\end{equation}
where severity $\in [0,1]$ weights the shock magnitude. Uncertainty inflates by factor 1.5--2.0× to reflect model uncertainty in shock scenarios.

\subsection{Betting Performance with Causal Adjustments}

\Cref{tab:causal-betting-backtest} presents preliminary backtest results comparing baseline and causal-adjusted predictions on 147 shock events (2020--2024).

\input{../figures/out/causal_betting_backtest_table.tex}

\paragraph{Key Results.}
\begin{itemize}
  \item \textbf{Prediction accuracy}: Causal-adjusted MAE: 16.2 yards vs baseline 18.7 yards (13.4\% improvement)
  \item \textbf{Win rate}: 56.8\% vs 53.2\% baseline (+3.6 percentage points)
  \item \textbf{ROI}: +5.1\% vs +1.8\% baseline (+3.3 percentage points)
  \item \textbf{Sample size}: Limited to 147 shocks; wide confidence intervals
\end{itemize}

\paragraph{By Event Type.}
Injury returns show strongest gains (62\% win rate, +8\% vs baseline). Coaching changes: 59\% (+6\%). Weather shocks: 58\% (+5\%). Trade effects: 54\% (+1\%, insufficient sample).

\paragraph{Caveats.}
Small sample sizes (n=147 total, 40--60 per category) preclude strong statistical claims. Requires multi-season validation before production deployment. Risk of overfitting to historical shock responses.

\subsection{Limitations and Future Work}

\paragraph{Current Limitations.}
\begin{enumerate}
  \item \textbf{Sample size}: Shock events rare; limited statistical power
  \item \textbf{BNN calibration}: Under-calibration issue requires architectural fixes before causal prior integration
  \item \textbf{Treatment heterogeneity}: Effects vary by player, team, context—not yet modeled
  \item \textbf{Time-varying confounding}: Some confounders evolve during treatment period
  \item \textbf{Spillover effects}: SUTVA violations (team adjustments, opponent responses)
\end{enumerate}

\paragraph{Future Enhancements.}
\begin{itemize}
  \item \textbf{Doubly-robust estimation}: Combine propensity scores with outcome regression for robustness
  \item \textbf{Causal forests}: Machine learning methods for heterogeneous treatment effects
  \item \textbf{Mediation analysis}: Decompose effects through specific mechanisms (e.g., carries → yards)
  \item \textbf{Dynamic treatment regimes}: Optimal sequential decisions under evolving information
  \item \textbf{Multi-sport generalization}: Extend framework to NBA/MLB injury markets
\end{itemize}

\paragraph{Integration Timeline.}
\begin{itemize}
  \item \textbf{Phase 1} (Complete): Framework implementation, validation suite
  \item \textbf{Phase 2} (Q1 2025): Fix BNN calibration, expand shock event database
  \item \textbf{Phase 3} (Q2 2025): Real-time shock detection, automated adjustment pipeline
  \item \textbf{Phase 4} (Q3 2025): Production deployment with risk gates
\end{itemize}

\subsection{Discussion: The Value of Negative Results}

Both the BNN calibration failures and limited shock event sample sizes represent valuable negative results that prevent premature deployment:

\paragraph{BNN Calibration.}
The prior sensitivity analysis definitively rules out one hypothesis (tight priors cause under-calibration) and redirects effort toward architectural improvements. This saves months of fruitless prior tuning.

\paragraph{Weather (Revisited).}
The causal framework confirms our earlier finding (\S\ref{subsec:weather-negative}) that weather effects are real but small ($-3.2$ points for extreme precipitation) and already priced by markets. This justifies removing weather features from high-frequency prediction paths.

\paragraph{Shock Event Rarity.}
Only 147 qualifying shock events over 5 seasons highlights the challenge: significant edges exist but opportunities are sparse. This informs capital allocation (shock strategies require patient capital) and justifies maintaining sophisticated infrastructure for rare events.

\subsection{Summary: Causal Inference as Competitive Advantage}

The causal inference framework demonstrates three contributions:

\begin{enumerate}
  \item \textbf{Methodological rigor}: Implements state-of-the-art causal inference methods (SC, DiD, DAGs) with validation frameworks
  \item \textbf{Practical integration}: Connects causal estimates to baseline models via features, priors, and adjustments
  \item \textbf{Honest reporting}: Documents failures (BNN calibration, limited samples) alongside successes
\end{enumerate}

While not yet production-ready, the framework establishes infrastructure for exploiting market inefficiencies that appear during regime breaks—the most promising frontier given semi-strong efficiency of typical game markets (\S\ref{sec:central-finding}). Causal reasoning transforms qualitative intuitions (``injuries matter'') into quantified, testable estimates with uncertainty bounds.

Future work focuses on fixing BNN calibration issues, expanding the shock event database, and validating that real-time deployment can capture the 3--6\% ROI improvements observed in historical backtests.
