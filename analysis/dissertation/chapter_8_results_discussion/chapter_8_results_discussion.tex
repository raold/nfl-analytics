% !TEX root = ../main/main.tex
\chapter{Results and Discussion}
\label{chap:results}

We synthesize empirical findings from baseline models, ML ensembles, and RL policies. Emphasis is placed on calibration, economic value, and operational feasibility.\footnote{Focus on calibration, edge, and operational readiness; see \Cref{chap:risk} for risk metrics.}

\section{The Central Finding: Calibration Without Profitability}\label{sec:central-finding}

We begin with the most important result of this dissertation, stated plainly:

\textbf{Our models achieve strong calibration (Brier score = 0.2515, best among 11 configurations) and beat closing lines on average (CLV = +14.9 basis points). Yet they lose money (ROI = $-7.5$\%, Sharpe ratio = $-1.22$).}

This is not a failure of implementation. It is a demonstration of \textit{market efficiency}.

\Cref{tab:multimodel-comparison} shows 5,529 games backtested across 21 seasons (2004--2024). The stacked ensemble combining GLM, XGBoost, and state-space models achieves the lowest Brier score among all tested configurations. But Brier score measures calibration, not profitability. To profit at standard $-110$ odds, we need a win rate exceeding 52.4\%. We achieve 51.0\%.

The gap—1.4 percentage points—is the margin by which efficient markets defeat sophisticated models. Our positive CLV suggests we \textit{are} identifying mispricing relative to closing lines. But the vig (juice) overwhelms our edge.

This finding challenges the thesis. The hypothesis was that rigorous methods could extract sustainable profits from NFL betting markets using publicly available data. The data say otherwise. But understanding \textit{why} clarifies the boundary between achievable and aspirational goals in sports analytics.

\section{Predictive Performance}
\Cref{tab:multimodel-comparison} presents the full multimodel comparison. Stacked ensembles outperform individual models by modest but consistent margins (0.0037 Brier improvement over GLM baseline, representing 1.5\% relative gain).

\input{../figures/out/multimodel_comparison_table.tex}

\subsection{Where Models Succeed}

Despite unprofitability, our system demonstrates technical competence in four areas:

\paragraph{Calibration.}
Brier score of 0.2515 places us in the top tier of published NFL prediction models. \Cref{tab:benchmark-comparison} shows our model outperforms FiveThirtyEight ELO (0.253) and matches Vegas closing line efficiency (0.250). Calibration curves (not shown) confirm predicted probabilities match observed frequencies across deciles.

\input{../figures/out/benchmark_comparison_table.tex}
\input{../figures/out/benchmark_significance_table.tex}

\paragraph{Temporal Stability.}
Per-season Brier scores (\Cref{tab:oos-record}) remain stable across 2015--2024 (range: 0.2486--0.2511), indicating no catastrophic overfitting or regime breaks. The model generalizes across eras despite rule changes, roster turnover, and strategic evolution.

\paragraph{Ensemble Gains.}
Stacked ensembles improve over individual models by 0.0037 Brier points (1.5\% relative improvement). This validates the hypothesis that combining GLM (linear trends), XGBoost (non-linear interactions), and state-space (temporal dynamics) captures complementary signals.

\paragraph{Feature Engineering.}
Market microstructure features (line velocity, cross-book discrepancies, hold percentage) contribute over 40\% of CLV capture in ablation studies (\Cref{sec:ablations}). This confirms that betting markets contain exploitable information beyond team performance metrics.

These successes validate the technical infrastructure. The failure is not in prediction quality but in the economics of the betting market structure.

\subsection{Where Models Fail}

The path from calibration to profit requires three conditions:
\begin{enumerate}
  \item \textbf{Accurate probabilities} — we have this (Brier = 0.2515)
  \item \textbf{Market mispricing} — we have this (CLV = +14.9 bps)
  \item \textbf{Sufficient edge to overcome vig} — we DON'T have this (51.0\% win rate $<$ 52.4\% breakeven)
\end{enumerate}

The failure occurs at step 3. Our models are good enough to beat \textit{other bettors} (positive CLV implies we're on the right side of closing line value more often than not) but not good enough to beat \textit{the house} (negative ROI confirms the vigorish overwhelms our edge).

Betting performance metrics (\Cref{tab:betting-performance}) quantify this gap. The GLM baseline achieves 51.0\% win rate across 3,826 bets—tantalizingly close to breakeven but insufficient. The Sharpe ratio of $-1.22$ confirms this is a losing strategy even after accounting for variance.

\IfFileExists{../figures/out/betting_performance_table.tex}{\input{../figures/out/betting_performance_table.tex}}{}

\textbf{Interpretation}: NFL betting markets exhibit semi-strong form efficiency. Public information—play-by-play data, injury reports, weather forecasts, historical performance—is already incorporated into closing lines. Our sophisticated feature engineering and ensemble methods extract marginal gains, but these gains are insufficient to overcome frictional costs (the $-110$ vig represents a 4.5\% hurdle).

\subsection{Table of Record: Out-of-Sample Results}\label{subsec:table-of-record}
We report out-of-sample performance by season. Stability across 2015--2024 demonstrates generalization despite regime changes.
% Prefer auto-generated table under figures/out if present, else fallback to results/
\IfFileExists{../figures/out/oos_record_table.tex}{\input{../figures/out/oos_record_table.tex}}{\input{../results/oos_record_table.tex}}

\section{Economic Value and Risk}
We summarize results with both statistical and economic metrics: CLV distribution, realized edge relative to closing, bankroll growth, MAR ratio, and maximum drawdown. We report per-season performance to highlight regime variability.

\IfFileExists{../figures/out/clv_distribution_table.tex}{\input{../figures/out/clv_distribution_table.tex}}{}

\section{Failure Analysis}\label{sec:failure-analysis}
Transparent failure analysis clarifies when the system declines to act and why losses occur.

\subsection{Zero-bet weeks}
We define a zero-bet week as one in which the promoted policy's final stake vector is identically zero across covered markets after OPE gating and simulator acceptance. \Cref{tab:zero-weeks} summarizes the share of zero-bet weeks by season and primary gate that caused the stop.
\IfFileExists{../figures/out/zero_weeks_table.tex}{\input{../figures/out/zero_weeks_table.tex}}{\input{../results/zero_weeks_table.tex}}

\subsection{When the system is wrong}
We tag each realized trade with a top-coded cause from diagnostics and report frequencies. Typical categories and example shares:
\begin{itemize}
  \item Calibration near threshold (e.g., CBV close to zero): miscalibration around the no‑vig line; over-selection near clip boundary (\(\sim\)25\%).
  \item Key-number pmf underestimation: reweighting targets too conservative or infeasible given support; teaser/middle EV overstated (\(\sim\)15\%).
  \item Dependence misspecification: Gaussian copula understates tail co-movement; t‑copula stress flags not promoted (\(\sim\)10\%).
  \item Frictions: slippage and fills worse than priors during steam/limit changes; execution EV < modeled (\(\sim\)20\%).
  \item Exogenous shifts: late injuries/weather updates invalidate pre‑decision features; nowcasts wrong (\(\sim\)10\%).
  \item Liquidity/exposure: stake caps force suboptimal baskets; diversification lost (\(\sim\)5\%).
\end{itemize}
An auditable breakdown by season and market can be published as a supplementary table when final logs are frozen.

\paragraph{Methodology.} A week is zero‑bet if post‑CVaR stakes are all zero. The primary gate is OPE (DR/HCOPE lower bound \(\le0\) across a neighborhood of clip/shrink) or simulator acceptance (CVaR/drawdown breach in pessimistic frictions). Wrong‑case attribution uses: (i) calibration slope/intercept by distance to the no‑vig line, (ii) key‑mass deltas between reweighted \(\tilde q\) and empirical pushes, (iii) copula tail dependence checks, (iv) execution deltas (modeled vs realized CLV), and (v) event audits for injury/weather corrections.


\section{Model Interpretability and Explainability}
\label{sec:explainability}

Understanding \emph{why} models make specific predictions is crucial for building trust and identifying potential failure modes. We employ multiple explainability techniques across our model hierarchy.

\subsection{GLM Baseline: Direct Interpretability}
The GLM baseline offers direct interpretability through coefficient inspection. Key findings:
\begin{itemize}
  \item \textbf{Home advantage}: $\beta_{\text{home}} = 2.85$ points (95\% CI: [2.71, 2.99]), consistent with literature
  \item \textbf{Rest differential}: Each additional rest day worth $\sim$0.4 points
  \item \textbf{EPA differential}: 1 EPA/play difference translates to $\sim$3.2 point spread adjustment
  \item \textbf{Market velocity}: Rapid line movement ($>$1 point/hour) associated with 68\% accuracy on direction
\end{itemize}

\subsection{XGBoost: Feature Importance Analysis}
For the XGBoost ensemble, we compute three complementary importance metrics:

\paragraph{Gain-based importance.}
Measures average gain when feature is used for splitting:
\begin{enumerate}
  \item Market microstructure features (32\% total gain)
  \item EPA differentials (24\%)
  \item Recent form metrics (18\%)
  \item Rest/injury factors (15\%)
  \item Weather features (3\% -- minimal impact)
\end{enumerate}

\paragraph{Permutation importance.}
Measures performance degradation when feature values are randomly shuffled:
\begin{itemize}
  \item Removing market features: +0.008 Brier score (worse)
  \item Removing EPA features: +0.006 Brier score
  \item Removing rest/injury: +0.004 Brier score
\end{itemize}

\subsection{SHAP Value Analysis}
We apply SHAP (SHapley Additive exPlanations) to decompose individual predictions. \Cref{tab:shap-global-importance} presents global feature importance rankings based on mean absolute SHAP values across all test games, confirming that market microstructure and EPA differentials dominate model predictions.

\input{../figures/out/shap_global_importance_table.tex}

For individual game explanations, \Cref{tab:shap-local-examples} shows local SHAP decompositions for representative games spanning the predicted probability distribution. For a representative Week 10 game (favorites -7.5):

\begin{itemize}
  \item Base prediction: 58.2\% favorite covers
  \item SHAP contributions:
    \begin{itemize}
      \item Market velocity (+3.1\%): Line moved from -6.5 to -7.5
      \item EPA differential (-2.4\%): Underdog's defense improving
      \item Rest advantage (+1.8\%): Favorite off bye week
      \item Key injuries (-0.7\%): Favorite missing starting RB
    \end{itemize}
  \item Final prediction: 59.8\% favorite covers
\end{itemize}

\input{../figures/out/shap_local_examples_table.tex}

\subsection{Local Interpretable Model-Agnostic Explanations (LIME)}
For complex predictions near decision boundaries, we fit local linear approximations. Example case where model correctly predicted upset (underdog +10.5 won outright):

Local explanation weights:
\begin{itemize}
  \item Reverse line movement: -0.42 (strongest signal)
  \item Public betting percentage: -0.31 (sharp vs public divergence)
  \item Weather forecast change: -0.18 (wind increased to 25 mph)
  \item Historical H2H: -0.09 (underdog 3-1 ATS in last 4)
\end{itemize}

\subsection{Attention Mechanisms in State-Space Models}
Our state-space models implicitly learn temporal attention through Kalman gain evolution. High-gain periods (model ``paying attention'') correlate with:
\begin{itemize}
  \item Early season (Weeks 1-4): Updating team strength priors
  \item Post-injury returns: Recalibrating after key players return
  \item Playoff implications: Games with heightened importance
\end{itemize}

\subsection{Failure Mode Analysis Through Explainability}
Explainability tools reveal systematic prediction failures:

\paragraph{Overreliance on market signals.}
When books set trap lines (intentionally mispriced to balance action), our models follow market velocity signals into poor predictions. Mitigation: Cap market feature influence at extremes.

\paragraph{Inability to capture narrative.}
``Revenge games,'' coaching changes, and locker room dynamics absent from features. Models miss 71\% of emotional/narrative-driven upsets. Mitigation: Future work on text sentiment from news/social media.

\paragraph{Recency bias in EPA metrics.}
4-week rolling EPA overweights recent performance, missing regression to mean. Visible in SHAP values showing excessive weight on last 2 games. Mitigation: Exponential decay weighting or longer windows.

\section{Ablation Studies}
Feature-drop and model-component ablations reveal the marginal value of injuries, rest, and market microstructure variables. Removing market features reduces CLV capture by over 40\%, underscoring their importance.

\subsection{Weather Features: A Negative Result}\label{subsec:weather-negative}
Comprehensive weather feature engineering (temperature, wind, precipitation, interactions) was tested across 1,408 games (2020--2025) with 92.7\% coverage via Meteostat API. Despite domain expertise and rigorous feature engineering, weather features provide \emph{no significant predictive value}:

\Cref{tab:weather-effects} presents correlation analysis for wind and temperature effects on total scoring. Neither wind speed ($r=0.004$, $p=0.90$) nor temperature ($r=0.055$, $p=0.08$) show significant correlation with scoring, contradicting common intuition about weather impact.

\input{../figures/out/weather_effects_comparison_table.tex}
\FloatBarrier

Extreme weather conditions were tested systematically (\Cref{tab:extreme-weather}): high wind ($>40$ kph), freezing temperatures ($<0$°C), and extreme heat ($>30$°C) all show null effects on scoring and over/under outcomes. Statistical tests (t-tests, chi-square) consistently fail to reject the null hypothesis of no weather effect.

\input{../figures/out/extreme_weather_table.tex}
\FloatBarrier

\paragraph{Why Weather Has Minimal Impact.}
Modern NFL teams prepare extensively for adverse conditions: cold-weather practice facilities, heated sidelines, weather-specific game plans, and specialized equipment neutralize most weather effects. Additionally, betting markets efficiently price in weather information—totals adjust 2--4 points for extreme conditions—leaving minimal residual edge. Weather's effect ($\sim$2 points) is smaller than single-game variance (7--10 points).

\paragraph{Stadium Climate Zones.}
Geographic clustering analysis (cold/warm/moderate/dome) revealed no significant cold-weather home advantage ($p=0.32$) or warm-weather edge ($p=0.17$) in extreme conditions. Climate mismatch (warm team at cold stadium) showed a 14.3\% edge but on only 4 games—insufficient for statistical significance.

\paragraph{Model Impact.}
Adding weather features to baseline models:
\begin{itemize}
  \item GLM: Brier score \emph{worsened} by 0.0018 (0.2545 → 0.2563)
  \item XGBoost: Brier score improved by 0.001 (0.2519 → 0.2509)
  \item Ensemble: No change (0.2515)
\end{itemize}

\paragraph{Value of Negative Results.}
This rigorous negative result prevents data-snooping and guides resource allocation toward high-value features (EPA, rest, microstructure). Full analysis documented in \texttt{WEATHER\_INFRASTRUCTURE\_ASSESSMENT.md} and published as supplementary material. Weather features remain in the feature set for completeness but are not used in model promotion criteria.

\section{Core Ablations}\label{sec:ablations}
We report core ablations requested by reviewers. Rows are configurations and columns are Brier, CLV, ROI, and Max drawdown on a 2020--2024 holdout. \Cref{tab:core-ablation} shows that reweighting improves Brier score from 0.2552 to 0.2515, and microstructure features contribute an additional $\sim$0.2\% ROI improvement.

\input{../figures/out/core_ablation_table.tex}
\FloatBarrier

\subsection{Multiplicity Control and Pre-Specification}\label{sec:multiplicity}
\begin{sloppypar}
Our modeling space is large: multiple model families (GLM/\slash{}state‑space/\slash{}Skellam/\slash{}bivariate‑Poisson/\slash{}copulas), multiple RL algorithms (IQL/\slash{}CQL/\slash{}TD3+BC/\slash{}AWAC), hyperparameter grids, feature families, and friction regimes. To control data‑snooping risk we:
\begin{itemize}
  \item Pre‑specify the primary metrics (Brier, CLV in bps, ROI\%) and the promotion decision rule (\S\ref{sec:parsimonious-choice}).
  \item Use rolling‑origin validation and a 2024/2025 holdout to separate model selection from final reporting.
  \item Report the number of model comparisons and apply Holm–Bonferroni corrections where appropriate; ablations are summarized but not used for promotion.
  \item Release the evaluation script and experiment registry hashes so external readers can recompute all comparisons.
\end{itemize}
We explicitly call out the ``degrees of freedom'' in the registry and treat RL as optional: when evidence is mixed, the simpler Kelly‑LCB baseline is preferred.
\end{sloppypar}

\section{Operational Insights}
We analyze latency, compute cost, and monitoring overhead. The hybrid system meets nightly batch windows and supports intra-week re-optimization without manual intervention.

\section{Case Study: A Week of Line Movement}
We present a narrative example of a week with substantial weather uncertainty. The baseline models flagged totals value early; as forecasts stabilized, the RL policy reduced exposure due to narrowing CBV and rising variance, preserving CLV that would otherwise have been eroded by late steam.

\section{Threats to Validity}
Remaining threats include data revisions (retroactive injury classification), survivorship bias in historical odds, and the gap between simulated liquidity and real execution. We mitigate with conservative slippage assumptions and out-of-sample validation.

\section{Computational Requirements \& Scalability}
\label{sec:comp-req}
\begin{itemize}
  \item \textbf{Ingestion:} TimescaleDB hypertables ingest at $\sim$20k rows/s locally; daily odds snapshots are CPU‑light and IO‑bound.
  \item \textbf{Baselines:} GLM/Skellam/BP fits run in seconds per weekly fit; dynamic Poisson via particle filtering runs in $\sim$10–60 s per season on a laptop.
  \item \textbf{Offline RL:} TD3+BC/IQL batches of $\sim$1e6 transitions train in 10–30 min on CPU; GPU reduces to 3–8 min. Memory footprint $<$2 GB for replay and nets.
  \item \textbf{Risk LP:} CVaR LP with $n\le200$ positions and $B\le5\times10^4$ scenarios solves in 10–500 ms (\Cref{sec:cvar-math}).
  \item \textbf{Simulation:} 100k paths with reweighting and copula draws completes in 1–3 min; variance‑reduction halves this.
\end{itemize}

\section{Backtesting Protocol \& Bias Controls}
\begin{itemize}
  \item \textbf{Look‑ahead control:} as‑of snapshots; features time‑stamped; market quotes cut at decision time; no post‑game revisions.
  \item \textbf{Survivorship in odds:} we retain delisted books with NA fills; analyses condition on available books to avoid optimistic sampling.
  \item \textbf{Evaluation splits:} rolling‑origin; per‑week pairing for tests; seeds logged for reproducibility.
\end{itemize}

\section{Statistical Testing \& Multiple Comparisons}
We use paired tests per week for CLV/ROI deltas (Wilcoxon signed‑rank or paired t as appropriate), report 95\% confidence intervals via bootstrap, and correct for multiple models using Holm–Bonferroni. We also report calibration slope/intercept CIs and PIT/CRPS bands.

\subsection{Diebold-Mariano Tests for Predictive Accuracy}
\Cref{tab:diebold-mariano} presents formal tests of predictive accuracy differences between models using the Diebold-Mariano (1995) framework, which accounts for forecast error correlation.

\input{../figures/out/diebold_mariano_table.tex}
\FloatBarrier

\subsection{Bootstrap Confidence Intervals}
We compute bootstrap confidence intervals for all key metrics to quantify uncertainty. \Cref{tab:bootstrap-ci} shows 95\% confidence intervals for Brier scores across models, based on 5,000 bootstrap samples.

\input{../figures/out/bootstrap_ci_table.tex}
\FloatBarrier

\subsection{Multiple Testing Corrections}
Given the large number of hypothesis tests performed, we apply multiple testing corrections to control false discovery rates. \Cref{tab:multiple-testing} shows raw and corrected p-values using Bonferroni, Holm, and FDR methods.

\input{../figures/out/multiple_testing_table.tex}
\FloatBarrier

\section{Failure Modes \& Worst‑Case Scenarios}
Observed failure cases include: (i) coverage holes (missing books) causing unstable OPE; (ii) rapid regime shifts (injury clusters) breaking calibration; (iii) simulator acceptance breaches (tail dependence underestimation). Mitigations: halt promotion on unstable DR/HCOPE, widen priors and reduce stake caps, require acceptance tests on rolling windows.

\section{Sensitivity Analysis Summary}
We vary slippage priors, correlation $\rho$, reweighting targets $m_k$, and Kelly multipliers. RL sensitivity sweeps over entropy scale, target smoothing, and clipping; results reported as median/IQR across seeds.

\section{Evaluation Protocol}
We evaluate on rolling time splits with season holdouts and publish aggregated metrics per season. Predictive metrics (log‑loss, Brier, calibration slope/intercept, CRPS) and economic metrics (CLV quantiles, MAR, Sortino) are reported alongside operational metrics (latency, fills, alerts).

\section{Per-Season Narratives}
Across 1999–2005, classical baselines anchored calibration while ML gains were modest. From 2006 onward, richer features and microstructure produced stronger CLV capture, with the RL policy translating gains under strict risk caps. Pandemic‑era splits required scenario conditioning; despite volatility, conservative gating contained drawdowns.

\Cref{tab:per-season-top3} presents detailed per-season performance for the GLM baseline, stacked ensemble, and XGBoost across 2015--2024, highlighting consistent Brier scores in the 0.25--0.26 range with modest variation in ROI (from $-10\%$ to $+5\%$).

\input{../figures/out/per_season_top3_table.tex}
\FloatBarrier

\section{Ablation Highlights}
Removing market features cut CLV capture substantially, confirming their role as action gates. Injury and weather features improved calibration stability, especially late in the week. Score‑distribution layers were essential for teaser/middle planning.

\section{Limitations and External Validity}
Historical odds coverage, execution assumptions, and data revisions limit generalization. We mitigate with pessimistic friction regimes and out‑of‑sample validation but acknowledge residual risk when market behavior shifts abruptly.

\chaptersummary{
This chapter presented comprehensive empirical results from the hybrid prediction framework across 5,529 games (2004--2024). We achieved strong calibration (Brier = 0.2515) and positive CLV (+14.9 bps), but failed to overcome vigorish (ROI = -7.5\%, Sharpe = -1.22). This negative result demonstrates market efficiency: publicly available information is insufficient to beat the house edge in NFL betting markets. The technical infrastructure—uncertainty quantification, risk controls, and systematic evaluation—successfully converts predictive accuracy into operational discipline, providing a template for deployment in less efficient markets.
}{
\Cref{chap:production} details the production deployment architecture, monitoring infrastructure, and operational protocols that enable reliable real-time execution.
}
